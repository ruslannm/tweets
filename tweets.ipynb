{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJaxYQV3Pjxp",
        "outputId": "770095c1-eeea-4572-c7c5-543a828d866e"
      },
      "source": [
        "# !pip3 install --upgrade gensim --user"
      ],
      "id": "VJaxYQV3Pjxp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph_pq6aA9iOH",
        "outputId": "e7de7596-891d-4206-9c3e-10d15e99db27"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "id": "Ph_pq6aA9iOH",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6b483e3"
      },
      "source": [
        "import logging\n",
        "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import words\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from nltk.util import ngrams\n",
        "from time import time\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from nltk.metrics.distance  import edit_distance"
      ],
      "id": "b6b483e3",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bf0e0f5"
      },
      "source": [
        "# For Colab\n",
        "# PATH = '/content/drive/MyDrive/Colab Notebooks/21/tweets/data/'\n",
        "# NLTK_PATH = '/content/drive/MyDrive/Colab Notebooks/21/tweets/nltk_data/'\n",
        "\n",
        "# For home\n",
        "PATH = './data/'\n",
        "NLTK_PATH = './nltk_data/'\n",
        "pd.set_option(\"max_colwidth\", 400)"
      ],
      "id": "9bf0e0f5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86e29616"
      },
      "source": [
        "# Replace unencodable character by questionmark"
      ],
      "id": "86e29616"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd93805c"
      },
      "source": [
        "def sanitize_characters(input_file, output_file):    \n",
        "    with open(file=PATH+input_file, mode=\"r\", encoding='utf-8', errors='replace') as f_in:\n",
        "        data = f_in.read()\n",
        "        with open(file=PATH+output_file, mode=\"w\", encoding='utf-8') as f_out:\n",
        "            f_out.write(data)\n",
        "\n",
        "sanitize_characters(\"processedNegative.csv\", \"negative_clean\")\n",
        "sanitize_characters(\"processedNeutral.csv\", \"neutral_clean\")\n",
        "sanitize_characters(\"processedPositive.csv\", \"positive_clean\")"
      ],
      "id": "dd93805c",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eed46715"
      },
      "source": [
        "# Read a file"
      ],
      "id": "eed46715"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2a14393"
      },
      "source": [
        "def get_data(input_file, sep, class_label):\n",
        "    with open(PATH+input_file, 'r') as f_in:\n",
        "        text = f_in.read()\n",
        "    df = pd.DataFrame(text.split(sep), columns=['Text'])\n",
        "    df['Class'] = class_label\n",
        "    return df\n",
        "\n",
        "negative = get_data(\"processedNegative.csv\", ' ,', -1)\n",
        "neutral = get_data(\"processedNeutral.csv\", ' ,', 0)"
      ],
      "id": "e2a14393",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "9928e7bd",
        "outputId": "7134b1aa-c097-48ad-8670-d8334df7ac62"
      },
      "source": [
        "negative.head()"
      ],
      "id": "9928e7bd",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                Text  Class\n",
              "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy     -1\n",
              "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy     -1\n",
              "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy     -1\n",
              "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy     -1\n",
              "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "34d37c5a",
        "outputId": "6b9927cb-4d8a-4c1b-c283-61d0fc9defeb"
      },
      "source": [
        "neutral.head()"
      ],
      "id": "34d37c5a",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pak PM survives removal scare, but court orders further probe into corruption charge.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why doctors, pharma companies are opposing names on</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                 Text  Class\n",
              "0                                                                                               Pak PM survives removal scare, but court orders further probe into corruption charge.      0\n",
              "1  Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.      0\n",
              "2                                                                                                       FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?      0\n",
              "3                                                                                                                                 Why doctors, pharma companies are opposing names on      0\n",
              "4                                                                                       Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.      0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf8edc3"
      },
      "source": [
        "## Save positive"
      ],
      "id": "6cf8edc3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abf69643"
      },
      "source": [
        "def join_beginspace(lst):\n",
        "    new_lst = []\n",
        "    i = 0\n",
        "    change = 0\n",
        "    while i < len(lst):\n",
        "        if lst[i] == '':\n",
        "            i += 1\n",
        "            continue\n",
        "        if i < len(lst)-1:\n",
        "            if lst[i+1] == '':\n",
        "                i += 1\n",
        "                continue\n",
        "            if lst[i+1][0] == ' ':\n",
        "                new_lst.append(lst[i]+lst[i+1])\n",
        "                i += 2\n",
        "                change = 1\n",
        "                continue\n",
        "        new_lst.append(lst[i])\n",
        "        i += 1\n",
        "    return(new_lst, change)\n",
        "\n",
        "\n",
        "def join_endspace(lst):\n",
        "    new_lst = []\n",
        "    i = 0\n",
        "    change = 0\n",
        "    while i < len(lst):\n",
        "        if lst[i] == '':\n",
        "            i += 1\n",
        "            continue\n",
        "        if i+1 < len(lst):\n",
        "            if lst[i+1] == '':\n",
        "                i += 1\n",
        "                continue\n",
        "            if (lst[i][-1] == ' '):\n",
        "                new_lst.append(lst[i]+lst[i+1])\n",
        "                i += 2\n",
        "                change = 1\n",
        "                continue;\n",
        "        new_lst.append(lst[i])\n",
        "        i += 1\n",
        "    return(new_lst, change)"
      ],
      "id": "abf69643",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d407c9a"
      },
      "source": [
        "def get_list(input_file, sep):\n",
        "    with open(PATH+input_file, 'r') as f_in:\n",
        "        text = f_in.read()\n",
        "    return text.split(sep)\n",
        "\n",
        "lst = get_list(\"processedPositive.csv\", ',')\n",
        "change = 1\n",
        "while(change == 1):\n",
        "    lst, change = join_beginspace(lst)\n",
        "change = 1\n",
        "while(change == 1):\n",
        "    lst, change = join_endspace(lst)\n",
        "positive = pd.DataFrame(lst, columns=['Text'])\n",
        "positive['Class'] = 1"
      ],
      "id": "3d407c9a",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "c82c188d",
        "outputId": "a6f672f6-79ee-49e7-dc4f-6a542908dbf2"
      },
      "source": [
        "positive.head()"
      ],
      "id": "c82c188d",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Good luck to Rich riding for great project in this Sunday. Can you donate?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Omg he... kissed... him crying with joy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                     Text  Class\n",
              "0          An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon      1\n",
              "1  Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...      1\n",
              "2  Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage      1\n",
              "3                              Good luck to Rich riding for great project in this Sunday. Can you donate?      1\n",
              "4                                                                 Omg he... kissed... him crying with joy      1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "c31b7d1f",
        "outputId": "7135c33e-f108-46f6-e2e2-0bcff1cc92ce"
      },
      "source": [
        "df = pd.concat([negative, neutral, positive], ignore_index=True)\n",
        "df.head()"
      ],
      "id": "c31b7d1f",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                Text  Class\n",
              "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy     -1\n",
              "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy     -1\n",
              "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy     -1\n",
              "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy     -1\n",
              "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "f235e265",
        "outputId": "05fd1547-00e9-4d14-9674-8f5f139cc776"
      },
      "source": [
        "df.tail()"
      ],
      "id": "f235e265",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2703</th>\n",
              "      <td>Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704</th>\n",
              "      <td>- top engaged members this week happy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>ngam to  weeks left for cadet pilot exam crying with joy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2706</th>\n",
              "      <td>Great! You're welcome Josh happy  ^Adam</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2707</th>\n",
              "      <td>Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                      Text  Class\n",
              "2703  Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this      1\n",
              "2704                                                 - top engaged members this week happy      1\n",
              "2705                              ngam to  weeks left for cadet pilot exam crying with joy      1\n",
              "2706                                               Great! You're welcome Josh happy  ^Adam      1\n",
              "2707                    Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon      1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "0f237278",
        "outputId": "6516277a-704e-4c50-905e-5a3e59bf375e"
      },
      "source": [
        "df.describe()"
      ],
      "id": "0f237278",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2708.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.057976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.819033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Class\n",
              "count  2708.000000\n",
              "mean      0.057976\n",
              "std       0.819033\n",
              "min      -1.000000\n",
              "25%      -1.000000\n",
              "50%       0.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2034b126"
      },
      "source": [
        "# cleaning text"
      ],
      "id": "2034b126"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "42841448",
        "outputId": "34999d25-e605-4468-a3bd-a63dfd40f768"
      },
      "source": [
        "def standardize_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
        "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
        "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \", regex=True)\n",
        "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
        "    df[text_field] = df[text_field].str.replace(r\",,\", \",\", regex=True)\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    return df\n",
        "\n",
        "df = standardize_text(df, 'Text')\n",
        "df.head()"
      ],
      "id": "42841448",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i miss going to gigs in liverpool unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there isnt a new riverdale tonight ? unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                Text  Class\n",
              "0  how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy     -1\n",
              "1                                                                                                                                                                                                                                                                          i miss going to gigs in liverpool unhappy     -1\n",
              "2                                                                                                                                                                                                                                                                       there isnt a new riverdale tonight ? unhappy     -1\n",
              "3                                                                                                                                                                                                          it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy     -1\n",
              "4                                                                                                                                                                who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391f02ae"
      },
      "source": [
        "### Data Overview"
      ],
      "id": "391f02ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "61175e17",
        "outputId": "23293d2f-6ff5-4915-fbdb-68fa31aed4d1"
      },
      "source": [
        "my_classes = df.Class.unique()\n",
        "df.groupby('Class').count()"
      ],
      "id": "61175e17",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>-1</th>\n",
              "      <td>834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Text\n",
              "Class      \n",
              "-1      834\n",
              " 0      883\n",
              " 1      991"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89f730a9"
      },
      "source": [
        "# Functions"
      ],
      "id": "89f730a9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d727b7c"
      },
      "source": [
        "def tokenize_text(text, approach):\n",
        "    if approach=='stemming':\n",
        "        ps =PorterStemmer()\n",
        "    elif approach=='lemmatization':\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "    elif approach=='stemming+misspelling':\n",
        "        ps = PorterStemmer()\n",
        "        correct_spellings = words.words()\n",
        "    elif approach=='lemmatization+misspelling':\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        correct_spellings = words.words()\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            if approach != 'just tokenization without stopwords' and word in stopwords.words('english'):\n",
        "                continue\n",
        "            if word[0] == \"'\" or word[0] in ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'):\n",
        "                continue\n",
        "            \n",
        "            if approach=='stemming':\n",
        "                tokens.append(ps.stem(word))\n",
        "            elif approach=='lemmatization':\n",
        "                tokens.append(lemmatizer.lemmatize(word))\n",
        "            elif approach=='stemming+misspelling':\n",
        "                after_stem = ps.stem(word)\n",
        "                if after_stem in correct_spellings:\n",
        "                    tokens.append(after_stem)\n",
        "                else:\n",
        "                    after_correct = [\n",
        "                        (jaccard_distance(set(ngrams(after_stem, 2)), \n",
        "                                          set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==after_stem[0]\n",
        "                        ]\n",
        "                    if after_correct:\n",
        "                        tokens.append(sorted(after_correct, key = lambda val:val[0])[0][1])\n",
        "                    else:\n",
        "                        print(word, after_stem)\n",
        "                        tokens.append(after_stem)\n",
        "            elif approach=='lemmatization+misspelling':\n",
        "                after_lemm = lemmatizer.lemmatize(word)\n",
        "                if after_lemm in correct_spellings:\n",
        "                    tokens.append(after_lemm)\n",
        "                else:\n",
        "                    after_correct = [\n",
        "                        (jaccard_distance(set(ngrams(after_lemm, 2)), \n",
        "                                          set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==after_lemm[0]\n",
        "                        ]\n",
        "                    if after_correct:\n",
        "                        tokens.append(sorted(after_correct, key = lambda val:val[0])[0][1])\n",
        "                    else:\n",
        "                        print(word, after_lemm)\n",
        "                        tokens.append(after_lemm)\n",
        "\n",
        "            else:\n",
        "                tokens.append(word)\n",
        "                \n",
        "    return tokens"
      ],
      "id": "5d727b7c",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05fa0c92"
      },
      "source": [
        "def get_matrix(sentence, mode):\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(sentence)\n",
        "    return t.texts_to_matrix(sentence, mode=mode)#'binary')"
      ],
      "id": "05fa0c92",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66df3677"
      },
      "source": [
        "def get_top(encoded_doc, df=df, number=10):\n",
        "    similarities = cosine_similarity(encoded_doc)\n",
        "    top_pair = list()\n",
        "    vector_size = encoded_doc.shape[0]\n",
        "    for i in range(vector_size):\n",
        "        for j in range(vector_size):\n",
        "            if i < j:\n",
        "                top_pair.append((i, j, similarities[i][j])) \n",
        "    return sorted(top_pair, key=lambda tweet: tweet[2], reverse=True)[0:number]   "
      ],
      "id": "66df3677",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5825acae"
      },
      "source": [
        "APPROACHES = ['just tokenization', 'stemming', 'lemmatization', \n",
        "            'stemming+misspelling', 'lemmatization+misspelling', 'just tokenization without stopwords']\n",
        "encoded_docs = pd.DataFrame(data = None, \n",
        "                            index=APPROACHES, \n",
        "                            columns=['0 or 1, if the word exists', 'word counts', 'TFIDF'])\n",
        "accuracy_matrix = encoded_docs.copy()\n",
        "roc_auc_matrix = encoded_docs.copy()\n",
        "y_predicted = encoded_docs.copy()\n",
        "mapping_columns = {'0 or 1, if the word exists':'binary', 'word counts':'count', 'TFIDF':'tfidf'}"
      ],
      "id": "5825acae",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7216e4",
        "outputId": "bf4e722b-7985-44a3-cc65-827fe52c0e91"
      },
      "source": [
        "nltk.data.path.append(NLTK_PATH)\n",
        "nltk.download('punkt', download_dir=NLTK_PATH)\n",
        "nltk.download('stopwords', download_dir=NLTK_PATH)\n",
        "nltk.download('wordnet', download_dir=NLTK_PATH)\n",
        "nltk.download('words', download_dir=NLTK_PATH)"
      ],
      "id": "6e7216e4",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /content/drive/MyDrive/Colab\n",
            "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /content/drive/MyDrive/Colab\n",
            "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /content/drive/MyDrive/Colab\n",
            "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /content/drive/MyDrive/Colab\n",
            "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a35b3c3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "list_labels = df['Class'].tolist()\n",
        "if os.path.isfile(PATH+'text_processed.csv') and os.access(PATH+'text_processed.csv', os.R_OK):\n",
        "    text_processed = pd.read_csv(PATH+'text_processed.csv').to_dict('list')\n",
        "    for approach in APPROACHES:\n",
        "        text_processed[approach] = np.array(text_processed[approach])\n",
        "        for mode in encoded_docs.columns.tolist():\n",
        "            encoded_docs.loc[approach,mode]=get_matrix(text_processed[approach], mapping_columns[mode])\n",
        "            X_train, X_test, y_train, y_test = train_test_split(encoded_docs.loc[approach,mode], list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
        "            lr = LogisticRegression()\n",
        "            lr.fit(X_train, y_train)\n",
        "            y_predicted_tmp = lr.predict(X_test)\n",
        "            y_predicted.loc[approach,mode] = y_predicted_tmp\n",
        "            accuracy_matrix.loc[approach,mode] = round(accuracy_score(y_test, y_predicted_tmp), 3)\n",
        "            roc_auc_matrix.loc[approach,mode] = round(roc_auc_score(label_binarize(y_test, classes=my_classes),\n",
        "                                                                    label_binarize(y_predicted_tmp, classes=my_classes),\n",
        "                                                                    average='macro',\n",
        "                                                                    multi_class='ovo'), 3)\n",
        "else:\n",
        "    text_processed = dict()\n",
        "    for approach in APPROACHES:\n",
        "        t = time()\n",
        "        text_processed[approach] = df.apply(lambda r: ' '.join(tokenize_text(r['Text'], approach)), axis=1).values\n",
        "        t = round((time() - t) / 60, 2)\n",
        "        print(f'Time get word matrix {approach}: {t} mins')\n",
        "        for mode in encoded_docs.columns.tolist():\n",
        "            encoded_docs.loc[approach,mode]=get_matrix(text_processed[approach], mapping_columns[mode])\n",
        "            X_train, X_test, y_train, y_test = train_test_split(encoded_docs.loc[approach,mode], list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
        "            lr = LogisticRegression()\n",
        "            lr.fit(X_train, y_train)\n",
        "            y_predicted_tmp = lr.predict(X_test)\n",
        "            y_predicted.loc[approach,mode] = y_predicted_tmp\n",
        "            accuracy_matrix.loc[approach,mode] = round(accuracy_score(y_test, y_predicted_tmp), 3)\n",
        "            roc_auc_matrix.loc[approach,mode] = round(roc_auc_score(label_binarize(y_test, classes=my_classes),\n",
        "                                                                    label_binarize(y_predicted_tmp, classes=my_classes),\n",
        "                                                                    average='macro',\n",
        "                                                                    multi_class='ovo'), 3)\n",
        "    pd.DataFrame(text_processed).to_csv(PATH+'text_processed.csv', index=False)"
      ],
      "id": "1a35b3c3",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "036aaf2b",
        "outputId": "83735263-59c4-44d9-f03e-70978757e086"
      },
      "source": [
        "accuracy_matrix"
      ],
      "id": "036aaf2b",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0 or 1, if the word exists</th>\n",
              "      <th>word counts</th>\n",
              "      <th>TFIDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>just tokenization</th>\n",
              "      <td>0.915</td>\n",
              "      <td>0.913</td>\n",
              "      <td>0.911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stemming</th>\n",
              "      <td>0.917</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatization</th>\n",
              "      <td>0.915</td>\n",
              "      <td>0.911</td>\n",
              "      <td>0.904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stemming+misspelling</th>\n",
              "      <td>0.924</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatization+misspelling</th>\n",
              "      <td>0.921</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>just tokenization without stopwords</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    0 or 1, if the word exists  ...  TFIDF\n",
              "just tokenization                                        0.915  ...  0.911\n",
              "stemming                                                 0.917  ...  0.906\n",
              "lemmatization                                            0.915  ...  0.904\n",
              "stemming+misspelling                                     0.924  ...  0.899\n",
              "lemmatization+misspelling                                0.921  ...  0.904\n",
              "just tokenization without stopwords                      0.932  ...  0.926\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "65c06770",
        "outputId": "27e140f2-04d8-4104-fb11-5e4b0088ae24"
      },
      "source": [
        "roc_auc_matrix"
      ],
      "id": "65c06770",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0 or 1, if the word exists</th>\n",
              "      <th>word counts</th>\n",
              "      <th>TFIDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>just tokenization</th>\n",
              "      <td>0.938</td>\n",
              "      <td>0.936</td>\n",
              "      <td>0.934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stemming</th>\n",
              "      <td>0.939</td>\n",
              "      <td>0.937</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatization</th>\n",
              "      <td>0.938</td>\n",
              "      <td>0.935</td>\n",
              "      <td>0.928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stemming+misspelling</th>\n",
              "      <td>0.944</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatization+misspelling</th>\n",
              "      <td>0.941</td>\n",
              "      <td>0.937</td>\n",
              "      <td>0.928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>just tokenization without stopwords</th>\n",
              "      <td>0.949</td>\n",
              "      <td>0.942</td>\n",
              "      <td>0.944</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    0 or 1, if the word exists  ...  TFIDF\n",
              "just tokenization                                        0.938  ...  0.934\n",
              "stemming                                                 0.939  ...   0.93\n",
              "lemmatization                                            0.938  ...  0.928\n",
              "stemming+misspelling                                     0.944  ...  0.924\n",
              "lemmatization+misspelling                                0.941  ...  0.928\n",
              "just tokenization without stopwords                      0.949  ...  0.944\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ca6b09",
        "outputId": "22c12acd-dbef-4b3b-aa4b-e57bc24e6db3"
      },
      "source": [
        "print('\\033[1m' +  f'Top 10 similar pairs of tweets' + '\\033[0m')\n",
        "for approach in APPROACHES:\n",
        "    for mode in encoded_docs.columns.tolist():\n",
        "        top10 = get_top(encoded_docs.loc[approach, mode], df, 10)\n",
        "        print('\\033[1m' + f'aprroach: {approach}, mode:{mode}' + '\\033[0m')\n",
        "        for pair in enumerate(top10):\n",
        "            print(\"%.0f. Similarity = %.3f\" % (pair[0] + 1, pair[1][2]))\n",
        "            print(f'  index={pair[1][0]}, {df.loc[pair[1][0], \"Text\"]}')\n",
        "            print(f'  index={pair[1][1]}, {df.loc[pair[1][1], \"Text\"]}')"
      ],
      "id": "e3ca6b09",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTop 10 similar pairs of tweets\u001b[0m\n",
            "\u001b[1maprroach: just tokenization, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "5. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: just tokenization, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "5. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: just tokenization, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=50, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "2. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=55, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "3. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=92, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "4. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=93, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "5. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=152, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "6. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=188, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "7. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=640, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "8. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=806, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "9. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=826, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "10. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=829, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "\u001b[1maprroach: stemming, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "5. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "6. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "7. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "8. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "9. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "10. Similarity = 1.000\n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "\u001b[1maprroach: stemming, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "5. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: stemming, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=370, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
            "  index=557, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
            "2. Similarity = 1.000\n",
            "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
            "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
            "3. Similarity = 1.000\n",
            "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
            "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
            "4. Similarity = 1.000\n",
            "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
            "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
            "5. Similarity = 1.000\n",
            "  index=160, what the fuck don 27t unhappy\n",
            "  index=361, fuck unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "7. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "10. Similarity = 1.000\n",
            "  index=360, instant message still hoping unhappy \n",
            "  index=728, instant message still hoping unhappy \n",
            "\u001b[1maprroach: lemmatization, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "2. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "3. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "4. Similarity = 1.000\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "5. Similarity = 1.000\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "6. Similarity = 1.000\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "7. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "8. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "9. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "10. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "\u001b[1maprroach: lemmatization, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "2. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "3. Similarity = 1.000\n",
            "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "4. Similarity = 1.000\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "5. Similarity = 1.000\n",
            "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "6. Similarity = 1.000\n",
            "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
            "7. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "8. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "9. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "10. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "\u001b[1maprroach: lemmatization, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=50, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "2. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=55, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "3. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=92, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "4. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=93, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "5. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=152, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "6. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=188, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "7. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=640, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "8. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=806, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "9. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=826, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "10. Similarity = 1.000\n",
            "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "  index=829, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
            "\u001b[1maprroach: stemming+misspelling, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "5. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "6. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "7. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "8. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "9. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "10. Similarity = 1.000\n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "\u001b[1maprroach: stemming+misspelling, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "5. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: stemming+misspelling, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
            "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
            "2. Similarity = 1.000\n",
            "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
            "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
            "3. Similarity = 1.000\n",
            "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
            "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
            "4. Similarity = 1.000\n",
            "  index=160, what the fuck don 27t unhappy\n",
            "  index=361, fuck unhappy\n",
            "5. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "6. Similarity = 1.000\n",
            "  index=360, instant message still hoping unhappy \n",
            "  index=728, instant message still hoping unhappy \n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: lemmatization+misspelling, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "5. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "6. Similarity = 1.000\n",
            "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "7. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "8. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "9. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "10. Similarity = 1.000\n",
            "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
            "\u001b[1maprroach: lemmatization+misspelling, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=200, i miss my baby unhappy \n",
            "  index=348, i miss my baby unhappy\n",
            "2. Similarity = 1.000\n",
            "  index=203, i miss jihoon sad\n",
            "  index=573, i miss jihoon sad\n",
            "3. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "4. Similarity = 1.000\n",
            "  index=308, i can't do it all on my own  (see what i did there? )\n",
            "  index=442, i can't do it all on my own  (see what i did there? )\n",
            "5. Similarity = 1.000\n",
            "  index=378, i miss quacktopia! unhappy\n",
            "  index=388, i miss quacktopia! unhappy\n",
            "6. Similarity = 1.000\n",
            "  index=383, penge damit unhappy\n",
            "  index=435, penge damit unhappy\n",
            "7. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=688, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: lemmatization+misspelling, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=465, so precious i really miss him unhappy  \n",
            "  index=693, so precious i really miss him unhappy  \n",
            "2. Similarity = 1.000\n",
            "  index=465, so precious i really miss him unhappy  \n",
            "  index=742, so precious i really miss him unhappy  \n",
            "3. Similarity = 1.000\n",
            "  index=693, so precious i really miss him unhappy  \n",
            "  index=742, so precious i really miss him unhappy  \n",
            "4. Similarity = 1.000\n",
            "  index=54, we don 27t talk anymore like we used to do unhappy\n",
            "  index=129, we don 27t talk anymore like we used to do unhappy\n",
            "5. Similarity = 1.000\n",
            "  index=201, it's only been a few days since their promotion ended but i'm already missing her so much unhappy \n",
            "  index=210, it's only been a few days since their promotion ended but i'm already missing her so much unhappy \n",
            "6. Similarity = 1.000\n",
            "  index=247, are dying of thirst  and it's all because of us unhappy  \n",
            "  index=379, are dying of thirst  and it's all because of us unhappy  \n",
            "7. Similarity = 1.000\n",
            "  index=370, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
            "  index=557, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
            "8. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=436, this is jimin to yoongi unhappy \n",
            "9. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=509, this is jimin to yoongi unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=429, this is jimin to yoongi unhappy \n",
            "  index=642, this is jimin to yoongi unhappy \n",
            "\u001b[1maprroach: just tokenization without stopwords, mode:0 or 1, if the word exists\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=1733, share the love  thanks for being top new followers this week happy  want this?\n",
            "  index=2433, share the love thanks for being top new followers this week happy   want this\n",
            "2. Similarity = 1.000\n",
            "  index=1743, hey thanks for being top new followers this week! much appreciated happy  want this ?\n",
            "  index=2622, hey thanks for being top new followers this week! much appreciated happy   want this ?\n",
            "3. Similarity = 1.000\n",
            "  index=1743, hey thanks for being top new followers this week! much appreciated happy  want this ?\n",
            "  index=2639, hey thanks for being top new followers this week! much appreciated happy   want this ?\n",
            "4. Similarity = 1.000\n",
            "  index=1751, thanks for connecting we are passionate about    hope you like our tweets happy\n",
            "  index=1967, thanks for connecting we are passionate about    hope you like our tweets happy\n",
            "5. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=1950, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
            "6. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=1976, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
            "7. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=2016, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
            "8. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=2032, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
            "9. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=2037, thanks for the recent follow happy to connect happy  have a great thursday   want this\n",
            "10. Similarity = 1.000\n",
            "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
            "  index=2068, thanks for the recent follow happy to connect happy  have a great thursday   want this ?\n",
            "\u001b[1maprroach: just tokenization without stopwords, mode:word counts\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
            "2. Similarity = 1.000\n",
            "  index=247, are dying of thirst  and it's all because of us unhappy  \n",
            "  index=379, are dying of thirst  and it's all because of us unhappy  \n",
            "3. Similarity = 1.000\n",
            "  index=1751, thanks for connecting we are passionate about    hope you like our tweets happy\n",
            "  index=1967, thanks for connecting we are passionate about    hope you like our tweets happy\n",
            "4. Similarity = 1.000\n",
            "  index=1838, share the love you're top engaged community members this week! much appreciated happy\n",
            "  index=2066, share the love you're top engaged community members this week! much appreciated happy\n",
            "5. Similarity = 1.000\n",
            "  index=1838, share the love you're top engaged community members this week! much appreciated happy\n",
            "  index=2235, share the love you're top engaged community members this week! much appreciated happy\n",
            "6. Similarity = 1.000\n",
            "  index=2066, share the love you're top engaged community members this week! much appreciated happy\n",
            "  index=2235, share the love you're top engaged community members this week! much appreciated happy\n",
            "7. Similarity = 1.000\n",
            "  index=2299, another great song  have a listen  its good! you may well like it happy\n",
            "  index=2303, another great song  have a listen  its good! you may well like it happy\n",
            "8. Similarity = 1.000\n",
            "  index=54, we don 27t talk anymore like we used to do unhappy\n",
            "  index=129, we don 27t talk anymore like we used to do unhappy\n",
            "9. Similarity = 1.000\n",
            "  index=141, her back unhappy \n",
            "  index=340, her back unhappy \n",
            "10. Similarity = 1.000\n",
            "  index=149, i miss you unhappy\n",
            "  index=756, miss you unhappy\n",
            "\u001b[1maprroach: just tokenization without stopwords, mode:TFIDF\u001b[0m\n",
            "1. Similarity = 1.000\n",
            "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "2. Similarity = 1.000\n",
            "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "3. Similarity = 1.000\n",
            "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "4. Similarity = 1.000\n",
            "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "5. Similarity = 1.000\n",
            "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "6. Similarity = 1.000\n",
            "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "7. Similarity = 1.000\n",
            "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "8. Similarity = 1.000\n",
            "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "9. Similarity = 1.000\n",
            "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "10. Similarity = 1.000\n",
            "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
            "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ba228b"
      },
      "source": [
        "# Visualization"
      ],
      "id": "d8ba228b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4066458"
      },
      "source": [
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(my_classes))\n",
        "    target_names = my_classes\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "id": "e4066458",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78837e71"
      },
      "source": [
        "def get_metrics(y_predicted, y_test):  \n",
        "    # true positives / (true positives+false positives)\n",
        "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
        "                                    average='weighted')             \n",
        "    # true positives / (true positives + false negatives)\n",
        "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
        "                              average='weighted')\n",
        "    \n",
        "    # harmonic mean of precision and recall\n",
        "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
        "    \n",
        "    # true positives + true negatives/ total\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
        "    accuracy, precision, recall, f1 = get_metrics(target, predictions)\n",
        "    print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
        "#     print('accuracy %s' % accuracy_score(target, predictions))\n",
        "    cm = confusion_matrix(target, predictions, labels=my_classes)\n",
        "    print('confusion matrix\\n %s' % cm)\n",
        "    print('(row=expected, col=predicted)')\n",
        "    \n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
      ],
      "id": "78837e71",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468c7b02"
      },
      "source": [
        "#  Best approach"
      ],
      "id": "468c7b02"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ea566a0"
      },
      "source": [
        "approach ='stemming+misspelling'\n",
        "mode = '0 or 1, if the word exists'"
      ],
      "id": "1ea566a0",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "14ed1c03",
        "outputId": "e3008c97-48e5-48d0-ea7f-6ff680c2a869"
      },
      "source": [
        "approach ='stemming+misspelling'\n",
        "mode = '0 or 1, if the word exists'\n",
        "evaluate_prediction(y_predicted.loc[approach, mode], y_test)"
      ],
      "id": "14ed1c03",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.924, precision = 0.930, recall = 0.924, f1 = 0.924\n",
            "confusion matrix\n",
            " [[152   9   6]\n",
            " [  1 175   1]\n",
            " [  1  23 174]]\n",
            "(row=expected, col=predicted)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "f5eecd44",
        "outputId": "d605fd72-b075-4844-f4c3-e15d8845434e"
      },
      "source": [
        "df.iloc[0,0]"
      ],
      "id": "f5eecd44",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy\""
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "9e33da56",
        "outputId": "5beb0ad5-7528-4a9a-8c49-a93d8c420fd9"
      },
      "source": [
        "text_processed[approach][0]"
      ],
      "id": "9e33da56",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'unhappily dog like though talk driver going said love go new york since trump probabl anybody know rand like fall dollar got money need chang keep get strong unhappily'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "85a0666b",
        "outputId": "5bb83488-883c-4fab-a765-63c25f83d530"
      },
      "source": [
        "text_processed['stemming'][0]"
      ],
      "id": "85a0666b",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'unhappi dog like though talk driver goingh said love go new york sinc trump probabl anybodi know rand like fall dollar got money need chang keep get stronger unhappi'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "a388ec9b",
        "outputId": "df4ce5de-aadf-4b66-bde0-28650d4a725d"
      },
      "source": [
        "text_processed['just tokenization'][0] "
      ],
      "id": "a388ec9b",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51c43e4c"
      },
      "source": [
        "# Word2Vec"
      ],
      "id": "51c43e4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8e48e30"
      },
      "source": [
        "## training the model"
      ],
      "id": "c8e48e30"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90c9631c",
        "outputId": "452103bc-599c-476b-d5da-302808d81e3f"
      },
      "source": [
        "import multiprocessing\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "id": "90c9631c",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-28 06:23:49,014 : INFO : 'pattern' package not found; tag filters are not available for English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd16388b"
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "cores = 1 if cores == 1 else cores - 1"
      ],
      "id": "fd16388b",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1144445c"
      },
      "source": [
        "The parameters:\n",
        "* min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "* window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* vector_size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
      ],
      "id": "1144445c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50f6fe91"
      },
      "source": [
        "## Building the Vocabulary Table:"
      ],
      "id": "50f6fe91"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71e7812d"
      },
      "source": [
        "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
      ],
      "id": "71e7812d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6d074b01",
        "outputId": "462bf328-4246-44a3-d07f-2175184b8921"
      },
      "source": [
        "approach = 'just tokenization'\n",
        "text_processed[approach][0]"
      ],
      "id": "6d074b01",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "92d981e4",
        "outputId": "d34a4f0d-f1e0-4928-c021-8418f2568f7c"
      },
      "source": [
        "t = time()\n",
        "sentences = [sentence.split() for sentence in text_processed[approach]]\n",
        "w2v_model = Word2Vec(sentences, vector_size=2000, min_count = 1, workers=cores-1)\n",
        "t = round((time() - t) / 60, 2)\n",
        "print(f'Time to build vocab: {t} mins')"
      ],
      "id": "92d981e4",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-dc200f8e70f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_processed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapproach\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Time to build vocab: {t} mins'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "2195d817"
      },
      "source": [
        "Example vocabulary"
      ],
      "id": "2195d817"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486432b3"
      },
      "source": [
        "w2v_model.wv.index_to_key[0:5]"
      ],
      "id": "486432b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28b932e4"
      },
      "source": [
        "Example vector"
      ],
      "id": "28b932e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a4f190d"
      },
      "source": [
        "word = w2v_model.wv.index_to_key[0:10][0]\n",
        "print(word)\n",
        "w2v_model.wv[word]"
      ],
      "id": "2a4f190d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c2ed07"
      },
      "source": [
        "Now we have a vector for each word. How do we get a vector for a sequence of words (aka a document)?\n",
        "The most naive way is just to take an average. "
      ],
      "id": "b8c2ed07"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f0431c8"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if word in wv.index_to_key:\n",
        "            if isinstance(wv[word], np.ndarray):\n",
        "                mean.append(wv[word])\n",
        "#         elif word in wv.vocab:\n",
        "#             mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "#             all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
      ],
      "id": "7f0431c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c53d6408"
      },
      "source": [
        "%%time\n",
        "X_word_average = word_averaging_list(w2v_model.wv, sentences)"
      ],
      "id": "c53d6408",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea64d972"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_word_average, list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
        "params = {'C':np.logspace(-3,3,7), 'penalty':['l1', 'l2']} # l1 lasso l2 ridge\n",
        "lr_gs = GridSearchCV(LogisticRegression(), params, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "lr_gs.fit(X_train, y_train)\n",
        "print('best parameters: ', lr_gs.best_params_)\n",
        "print('accuracy: ', lr_gs.best_score_)"
      ],
      "id": "ea64d972",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2tA9K5tGU1v"
      },
      "source": [
        "lr = LogisticRegression(C=lr_gs.best_params_['C'], penalty=lr_gs.best_params_['penalty'])\n",
        "y_predicted_w2v = lr.predict(X_test)\n",
        "evaluate_prediction(y_predicted_w2v, y_test)"
      ],
      "id": "c2tA9K5tGU1v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c548b79a"
      },
      "source": [
        "print('\\033[1m' +  f'Top 10 similar pairs of tweets' + '\\033[0m')\n",
        "top10 = get_top(X_word_average, df, 10)\n",
        "print('\\033[1m' + f'aprroach: Word2Vec' + '\\033[0m')\n",
        "for pair in enumerate(top10):\n",
        "    print(\"%.0f. Similarity = %.3f\" % (pair[0] + 1, pair[1][2]))\n",
        "    print(f'  index={pair[1][0]}, {df.loc[pair[1][0], \"Text\"]}')\n",
        "    print(f'  index={pair[1][1]}, {df.loc[pair[1][1], \"Text\"]}')"
      ],
      "id": "c548b79a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "078ccf5e"
      },
      "source": [
        ""
      ],
      "id": "078ccf5e",
      "execution_count": null,
      "outputs": []
    }
  ]
}