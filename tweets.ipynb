{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ph_pq6aA9iOH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph_pq6aA9iOH",
    "outputId": "7b94c4bb-14cb-49cc-921d-6c4d8307750e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /root/.local/lib/python3.7/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# !pip3 install - -upgrade gensim - -user\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vGuGb2ZZZi12",
   "metadata": {
    "id": "vGuGb2ZZZi12"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b483e3",
   "metadata": {
    "id": "b6b483e3"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from time import time\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.root.handlers = []\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf0e0f5",
   "metadata": {
    "id": "9bf0e0f5"
   },
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# PATH = '/content/drive/MyDrive/Colab Notebooks/21/tweets/data/'\n",
    "# NLTK_PATH = '/content/drive/MyDrive/Colab Notebooks/21/tweets/nltk_data/'\n",
    "\n",
    "# For other place\n",
    "PATH = './data/'\n",
    "NLTK_PATH = './nltk_data/'\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e29616",
   "metadata": {
    "id": "86e29616"
   },
   "source": [
    "# Replace unencodable character by questionmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd93805c",
   "metadata": {
    "id": "dd93805c"
   },
   "outputs": [],
   "source": [
    "def sanitize_characters(input_file, output_file):\n",
    "    with open(file=PATH+input_file, mode=\"r\", encoding='utf-8', errors='replace') as f_in:\n",
    "        data = f_in.read()\n",
    "        with open(file=PATH+output_file, mode=\"w\", encoding='utf-8') as f_out:\n",
    "            f_out.write(data)\n",
    "\n",
    "\n",
    "sanitize_characters(\"processedNegative.csv\", \"negative_clean\")\n",
    "sanitize_characters(\"processedNeutral.csv\", \"neutral_clean\")\n",
    "sanitize_characters(\"processedPositive.csv\", \"positive_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed46715",
   "metadata": {
    "id": "eed46715"
   },
   "source": [
    "# Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a14393",
   "metadata": {
    "id": "e2a14393"
   },
   "outputs": [],
   "source": [
    "def get_data(input_file, sep, class_label):\n",
    "    with open(PATH+input_file, 'r') as f_in:\n",
    "        text = f_in.read()\n",
    "    df = pd.DataFrame(text.split(sep), columns=['Text'])\n",
    "    df['Class'] = class_label\n",
    "    return df\n",
    "\n",
    "\n",
    "negative = get_data(\"processedNegative.csv\", ' ,', -1)\n",
    "neutral = get_data(\"processedNeutral.csv\", ' ,', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9928e7bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "9928e7bd",
    "outputId": "59c576db-a372-455a-e483-5e2dda30f962"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  Class\n",
       "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy     -1\n",
       "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy     -1\n",
       "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy     -1\n",
       "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy     -1\n",
       "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d37c5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "34d37c5a",
    "outputId": "0215f5ad-b761-4c46-de50-4e3a0a5feaec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pak PM survives removal scare, but court orders further probe into corruption charge.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why doctors, pharma companies are opposing names on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Text  Class\n",
       "0                                                                                               Pak PM survives removal scare, but court orders further probe into corruption charge.      0\n",
       "1  Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.      0\n",
       "2                                                                                                       FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?      0\n",
       "3                                                                                                                                 Why doctors, pharma companies are opposing names on      0\n",
       "4                                                                                       Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8edc3",
   "metadata": {
    "id": "6cf8edc3"
   },
   "source": [
    "## Save positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf69643",
   "metadata": {
    "id": "abf69643"
   },
   "outputs": [],
   "source": [
    "def join_beginspace(lst):\n",
    "    new_lst = []\n",
    "    i = 0\n",
    "    change = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == '':\n",
    "            i += 1\n",
    "            continue\n",
    "        if i < len(lst)-1:\n",
    "            if lst[i+1] == '':\n",
    "                i += 1\n",
    "                continue\n",
    "            if lst[i+1][0] == ' ':\n",
    "                new_lst.append(lst[i]+lst[i+1])\n",
    "                i += 2\n",
    "                change = 1\n",
    "                continue\n",
    "        new_lst.append(lst[i])\n",
    "        i += 1\n",
    "    return(new_lst, change)\n",
    "\n",
    "\n",
    "def join_endspace(lst):\n",
    "    new_lst = []\n",
    "    i = 0\n",
    "    change = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == '':\n",
    "            i += 1\n",
    "            continue\n",
    "        if i+1 < len(lst):\n",
    "            if lst[i+1] == '':\n",
    "                i += 1\n",
    "                continue\n",
    "            if (lst[i][-1] == ' '):\n",
    "                new_lst.append(lst[i]+lst[i+1])\n",
    "                i += 2\n",
    "                change = 1\n",
    "                continue\n",
    "        new_lst.append(lst[i])\n",
    "        i += 1\n",
    "    return(new_lst, change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d407c9a",
   "metadata": {
    "id": "3d407c9a"
   },
   "outputs": [],
   "source": [
    "def get_list(input_file, sep):\n",
    "    with open(PATH+input_file, 'r') as f_in:\n",
    "        text = f_in.read()\n",
    "    return text.split(sep)\n",
    "\n",
    "\n",
    "lst = get_list(\"processedPositive.csv\", ',')\n",
    "change = 1\n",
    "while(change == 1):\n",
    "    lst, change = join_beginspace(lst)\n",
    "change = 1\n",
    "while(change == 1):\n",
    "    lst, change = join_endspace(lst)\n",
    "positive = pd.DataFrame(lst, columns=['Text'])\n",
    "positive['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82c188d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "c82c188d",
    "outputId": "01910c7b-f1b2-4d55-831c-6ad0a060cc25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good luck to Rich riding for great project in this Sunday. Can you donate?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Omg he... kissed... him crying with joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     Text  Class\n",
       "0          An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon      1\n",
       "1  Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...      1\n",
       "2  Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage      1\n",
       "3                              Good luck to Rich riding for great project in this Sunday. Can you donate?      1\n",
       "4                                                                 Omg he... kissed... him crying with joy      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31b7d1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "c31b7d1f",
    "outputId": "9f497189-25b5-4b27-dc9e-96b1f4d904cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  Class\n",
       "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy     -1\n",
       "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy     -1\n",
       "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy     -1\n",
       "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy     -1\n",
       "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([negative, neutral, positive], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f235e265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "f235e265",
    "outputId": "e0d8def3-d2ef-4089-bd20-3306bee39a71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>- top engaged members this week happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam crying with joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>Great! You're welcome Josh happy  ^Adam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      Text  Class\n",
       "2703  Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this      1\n",
       "2704                                                 - top engaged members this week happy      1\n",
       "2705                              ngam to  weeks left for cadet pilot exam crying with joy      1\n",
       "2706                                               Great! You're welcome Josh happy  ^Adam      1\n",
       "2707                    Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f237278",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "0f237278",
    "outputId": "5a3db392-8b19-41c6-cc13-277b584e4de5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.057976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.819033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Class\n",
       "count  2708.000000\n",
       "mean      0.057976\n",
       "std       0.819033\n",
       "min      -1.000000\n",
       "25%      -1.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034b126",
   "metadata": {
    "id": "2034b126"
   },
   "source": [
    "# cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42841448",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "42841448",
    "outputId": "8fb51e7f-13c5-4220-c7e9-06c158812930"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there isnt a new riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  Class\n",
       "0  how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy     -1\n",
       "1                                                                                                                                                                                                                                                                          i miss going to gigs in liverpool unhappy     -1\n",
       "2                                                                                                                                                                                                                                                                       there isnt a new riverdale tonight ? unhappy     -1\n",
       "3                                                                                                                                                                                                          it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy     -1\n",
       "4                                                                                                                                                                who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad     -1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(\n",
    "        r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\",,\", \",\", regex=True)\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = standardize_text(df, 'Text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f02ae",
   "metadata": {
    "id": "391f02ae"
   },
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61175e17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "61175e17",
    "outputId": "1e3212b7-a6be-4553-a392-97c05736019c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text\n",
       "Class      \n",
       "-1      834\n",
       " 0      883\n",
       " 1      991"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classes = df.Class.unique()\n",
    "df.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f730a9",
   "metadata": {
    "id": "89f730a9"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d727b7c",
   "metadata": {
    "id": "5d727b7c"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text, approach):\n",
    "    if approach == 'stemming':\n",
    "        ps = PorterStemmer()\n",
    "    elif approach == 'lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    elif approach == 'stemming+misspelling':\n",
    "        ps = PorterStemmer()\n",
    "        correct_spellings = words.words()\n",
    "    elif approach == 'lemmatization+misspelling':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        correct_spellings = words.words()\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if approach != 'just tokenization without stopwords' and word in stopwords.words('english'):\n",
    "                continue\n",
    "            if word[0] == \"'\" or word[0] in ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'):\n",
    "                continue\n",
    "\n",
    "            if approach == 'stemming':\n",
    "                tokens.append(ps.stem(word))\n",
    "            elif approach == 'lemmatization':\n",
    "                tokens.append(lemmatizer.lemmatize(word))\n",
    "            elif approach == 'stemming+misspelling':\n",
    "                after_stem = ps.stem(word)\n",
    "                if after_stem in correct_spellings:\n",
    "                    tokens.append(after_stem)\n",
    "                else:\n",
    "                    after_correct = [\n",
    "                        (jaccard_distance(set(ngrams(after_stem, 2)),\n",
    "                                          set(ngrams(w, 2))), w) for w in correct_spellings if w[0] == after_stem[0]\n",
    "                    ]\n",
    "                    if after_correct:\n",
    "                        tokens.append(\n",
    "                            sorted(after_correct, key=lambda val: val[0])[0][1])\n",
    "                    else:\n",
    "                        print(word, after_stem)\n",
    "                        tokens.append(after_stem)\n",
    "            elif approach == 'lemmatization+misspelling':\n",
    "                after_lemm = lemmatizer.lemmatize(word)\n",
    "                if after_lemm in correct_spellings:\n",
    "                    tokens.append(after_lemm)\n",
    "                else:\n",
    "                    after_correct = [\n",
    "                        (jaccard_distance(set(ngrams(after_lemm, 2)),\n",
    "                                          set(ngrams(w, 2))), w) for w in correct_spellings if w[0] == after_lemm[0]\n",
    "                    ]\n",
    "                    if after_correct:\n",
    "                        tokens.append(\n",
    "                            sorted(after_correct, key=lambda val: val[0])[0][1])\n",
    "                    else:\n",
    "                        print(word, after_lemm)\n",
    "                        tokens.append(after_lemm)\n",
    "\n",
    "            else:\n",
    "                tokens.append(word)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05fa0c92",
   "metadata": {
    "id": "05fa0c92"
   },
   "outputs": [],
   "source": [
    "def get_matrix(sentence, mode):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(sentence)\n",
    "    return t.texts_to_matrix(sentence, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66df3677",
   "metadata": {
    "id": "66df3677"
   },
   "outputs": [],
   "source": [
    "def get_top(encoded_doc, df=df, number=10):\n",
    "    similarities = cosine_similarity(encoded_doc)\n",
    "    top_pair = list()\n",
    "    vector_size = encoded_doc.shape[0]\n",
    "    for i in range(vector_size):\n",
    "        for j in range(vector_size):\n",
    "            if i < j:\n",
    "                top_pair.append((i, j, similarities[i][j]))\n",
    "    return sorted(top_pair, key=lambda tweet: tweet[2], reverse=True)[0:number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5825acae",
   "metadata": {
    "id": "5825acae"
   },
   "outputs": [],
   "source": [
    "APPROACHES = ['just tokenization', 'stemming', 'lemmatization',\n",
    "              'stemming+misspelling', 'lemmatization+misspelling', 'just tokenization without stopwords']\n",
    "encoded_docs = pd.DataFrame(data=None,\n",
    "                            index=APPROACHES,\n",
    "                            columns=['0 or 1, if the word exists', 'word counts', 'TFIDF'])\n",
    "accuracy_matrix = encoded_docs.copy()\n",
    "roc_auc_matrix = encoded_docs.copy()\n",
    "y_predicted = encoded_docs.copy()\n",
    "mapping_columns = {'0 or 1, if the word exists': 'binary',\n",
    "                   'word counts': 'count', 'TFIDF': 'tfidf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e7216e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e7216e4",
    "outputId": "24755f08-d880-43b1-b1f6-96eba1d20513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /content/drive/MyDrive/Colab\n",
      "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /content/drive/MyDrive/Colab\n",
      "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /content/drive/MyDrive/Colab\n",
      "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /content/drive/MyDrive/Colab\n",
      "[nltk_data]     Notebooks/21/tweets/nltk_data/...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append(NLTK_PATH)\n",
    "nltk.download('punkt', download_dir=NLTK_PATH)\n",
    "nltk.download('stopwords', download_dir=NLTK_PATH)\n",
    "nltk.download('wordnet', download_dir=NLTK_PATH)\n",
    "nltk.download('words', download_dir=NLTK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a35b3c3",
   "metadata": {
    "id": "1a35b3c3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "list_labels = df['Class'].tolist()\n",
    "if os.path.isfile(PATH+'text_processed.csv') and os.access(PATH+'text_processed.csv', os.R_OK):\n",
    "    text_processed = pd.read_csv(PATH+'text_processed.csv').to_dict('list')\n",
    "    for approach in APPROACHES:\n",
    "        text_processed[approach] = np.array(text_processed[approach])\n",
    "        for mode in encoded_docs.columns.tolist():\n",
    "            encoded_docs.loc[approach, mode] = get_matrix(\n",
    "                text_processed[approach], mapping_columns[mode])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                encoded_docs.loc[approach, mode], list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
    "            params = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "            lr_gs = GridSearchCV(LogisticRegression(),\n",
    "                                 params, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "            lr_gs.fit(X_train, y_train)\n",
    "            y_predicted_tmp = lr_gs.predict(X_test)\n",
    "            y_predicted.loc[approach, mode] = y_predicted_tmp\n",
    "            accuracy_matrix.loc[approach, mode] = round(\n",
    "                accuracy_score(y_test, y_predicted_tmp), 3)\n",
    "            roc_auc_matrix.loc[approach, mode] = round(roc_auc_score(label_binarize(y_test, classes=my_classes),\n",
    "                                                                     label_binarize(\n",
    "                y_predicted_tmp, classes=my_classes),\n",
    "                average='macro',\n",
    "                multi_class='ovo'), 3)\n",
    "else:\n",
    "    text_processed = dict()\n",
    "    for approach in APPROACHES:\n",
    "        t = time()\n",
    "        text_processed[approach] = df.apply(lambda r: ' '.join(\n",
    "            tokenize_text(r['Text'], approach)), axis=1).values\n",
    "        t = round((time() - t) / 60, 2)\n",
    "        print(f'Time get word matrix {approach}: {t} mins')\n",
    "        for mode in encoded_docs.columns.tolist():\n",
    "            encoded_docs.loc[approach, mode] = get_matrix(\n",
    "                text_processed[approach], mapping_columns[mode])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                encoded_docs.loc[approach, mode], list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
    "            params = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "            lr_gs = GridSearchCV(LogisticRegression(),\n",
    "                                 params, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "            lr_gs.fit(X_train, y_train)\n",
    "            y_predicted_tmp = lr_gs.predict(X_test)\n",
    "            y_predicted.loc[approach, mode] = y_predicted_tmp\n",
    "            accuracy_matrix.loc[approach, mode] = round(\n",
    "                accuracy_score(y_test, y_predicted_tmp), 3)\n",
    "            roc_auc_matrix.loc[approach, mode] = round(roc_auc_score(label_binarize(y_test, classes=my_classes),\n",
    "                                                                     label_binarize(\n",
    "                y_predicted_tmp, classes=my_classes),\n",
    "                average='macro',\n",
    "                multi_class='ovo'), 3)\n",
    "    pd.DataFrame(text_processed).to_csv(PATH+'text_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "036aaf2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "036aaf2b",
    "outputId": "f9336e72-7d08-4c66-edfa-7940e03aac6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.902</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.917</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.915</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming+misspelling</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization+misspelling</th>\n",
       "      <td>0.921</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just tokenization without stopwords</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0 or 1, if the word exists  ...  TFIDF\n",
       "just tokenization                                        0.902  ...  0.917\n",
       "stemming                                                 0.917  ...  0.917\n",
       "lemmatization                                            0.915  ...  0.923\n",
       "stemming+misspelling                                     0.924  ...  0.919\n",
       "lemmatization+misspelling                                0.921  ...  0.913\n",
       "just tokenization without stopwords                      0.932  ...  0.926\n",
       "\n",
       "[6 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65c06770",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "65c06770",
    "outputId": "33cb5e96-5da0-4e94-ee88-0b64781ed0f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming+misspelling</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization+misspelling</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just tokenization without stopwords</th>\n",
       "      <td>0.949</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0 or 1, if the word exists  ...  TFIDF\n",
       "just tokenization                                        0.928  ...  0.939\n",
       "stemming                                                 0.939  ...  0.938\n",
       "lemmatization                                            0.938  ...  0.943\n",
       "stemming+misspelling                                     0.944  ...   0.94\n",
       "lemmatization+misspelling                                0.941  ...  0.936\n",
       "just tokenization without stopwords                      0.949  ...  0.945\n",
       "\n",
       "[6 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ca6b09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3ca6b09",
    "outputId": "577d69bf-e7e3-4fe5-d446-10c0e7fd6355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTop 10 similar pairs of tweets\u001b[0m\n",
      "\u001b[1maprroach: just tokenization, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "5. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: just tokenization, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "5. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: just tokenization, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=50, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "2. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=55, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "3. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=92, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "4. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=93, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "5. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=152, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "6. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=188, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "7. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=640, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "8. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=806, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "9. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=826, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "10. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=829, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "\u001b[1maprroach: stemming, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "5. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "6. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "7. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "8. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "9. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "10. Similarity = 1.000\n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "\u001b[1maprroach: stemming, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "5. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: stemming, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=370, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
      "  index=557, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
      "2. Similarity = 1.000\n",
      "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
      "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
      "3. Similarity = 1.000\n",
      "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
      "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
      "4. Similarity = 1.000\n",
      "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
      "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
      "5. Similarity = 1.000\n",
      "  index=160, what the fuck don 27t unhappy\n",
      "  index=361, fuck unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "7. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "10. Similarity = 1.000\n",
      "  index=360, instant message still hoping unhappy \n",
      "  index=728, instant message still hoping unhappy \n",
      "\u001b[1maprroach: lemmatization, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "2. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "3. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "4. Similarity = 1.000\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "5. Similarity = 1.000\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "6. Similarity = 1.000\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "7. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "8. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "9. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "10. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "\u001b[1maprroach: lemmatization, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "2. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "3. Similarity = 1.000\n",
      "  index=2210, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "4. Similarity = 1.000\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "5. Similarity = 1.000\n",
      "  index=2217, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "6. Similarity = 1.000\n",
      "  index=2243, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "  index=2247, good morning every one order what ever you want to eat through our website may you all have a great day happy\n",
      "7. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "8. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "9. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "10. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "\u001b[1maprroach: lemmatization, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=50, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "2. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=55, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "3. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=92, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "4. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=93, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "5. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=152, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "6. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=188, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "7. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=640, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "8. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=806, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "9. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=826, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "10. Similarity = 1.000\n",
      "  index=46, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "  index=829, yg should have sent them to mcd  i want to see them holding the trophy unhappy  anyways\n",
      "\u001b[1maprroach: stemming+misspelling, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "5. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "6. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "7. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "8. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "9. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "10. Similarity = 1.000\n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "\u001b[1maprroach: stemming+misspelling, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "5. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: stemming+misspelling, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
      "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
      "2. Similarity = 1.000\n",
      "  index=2158, thanks for the recent follow  much appreciated happy  get it\n",
      "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
      "3. Similarity = 1.000\n",
      "  index=2211, thanks for the recent follow  much appreciated happy  get this\n",
      "  index=2549, thanks for the recent follow  much appreciated happy  get it\n",
      "4. Similarity = 1.000\n",
      "  index=160, what the fuck don 27t unhappy\n",
      "  index=361, fuck unhappy\n",
      "5. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "6. Similarity = 1.000\n",
      "  index=360, instant message still hoping unhappy \n",
      "  index=728, instant message still hoping unhappy \n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: lemmatization+misspelling, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "5. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "6. Similarity = 1.000\n",
      "  index=304, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=593, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "7. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "8. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "9. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "10. Similarity = 1.000\n",
      "  index=401, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "  index=438, i only just absorbed that i'm not going to get to laugh when he gets 2000 votes  unhappy \n",
      "\u001b[1maprroach: lemmatization+misspelling, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=200, i miss my baby unhappy \n",
      "  index=348, i miss my baby unhappy\n",
      "2. Similarity = 1.000\n",
      "  index=203, i miss jihoon sad\n",
      "  index=573, i miss jihoon sad\n",
      "3. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "4. Similarity = 1.000\n",
      "  index=308, i can't do it all on my own  (see what i did there? )\n",
      "  index=442, i can't do it all on my own  (see what i did there? )\n",
      "5. Similarity = 1.000\n",
      "  index=378, i miss quacktopia! unhappy\n",
      "  index=388, i miss quacktopia! unhappy\n",
      "6. Similarity = 1.000\n",
      "  index=383, penge damit unhappy\n",
      "  index=435, penge damit unhappy\n",
      "7. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=688, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: lemmatization+misspelling, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=465, so precious i really miss him unhappy  \n",
      "  index=693, so precious i really miss him unhappy  \n",
      "2. Similarity = 1.000\n",
      "  index=465, so precious i really miss him unhappy  \n",
      "  index=742, so precious i really miss him unhappy  \n",
      "3. Similarity = 1.000\n",
      "  index=693, so precious i really miss him unhappy  \n",
      "  index=742, so precious i really miss him unhappy  \n",
      "4. Similarity = 1.000\n",
      "  index=54, we don 27t talk anymore like we used to do unhappy\n",
      "  index=129, we don 27t talk anymore like we used to do unhappy\n",
      "5. Similarity = 1.000\n",
      "  index=201, it's only been a few days since their promotion ended but i'm already missing her so much unhappy \n",
      "  index=210, it's only been a few days since their promotion ended but i'm already missing her so much unhappy \n",
      "6. Similarity = 1.000\n",
      "  index=247, are dying of thirst  and it's all because of us unhappy  \n",
      "  index=379, are dying of thirst  and it's all because of us unhappy  \n",
      "7. Similarity = 1.000\n",
      "  index=370, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
      "  index=557, a fish killer going to be pressed younhappy  i the hoe of today you were a  good person\n",
      "8. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=436, this is jimin to yoongi unhappy \n",
      "9. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=509, this is jimin to yoongi unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=429, this is jimin to yoongi unhappy \n",
      "  index=642, this is jimin to yoongi unhappy \n",
      "\u001b[1maprroach: just tokenization without stopwords, mode:0 or 1, if the word exists\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=1733, share the love  thanks for being top new followers this week happy  want this?\n",
      "  index=2433, share the love thanks for being top new followers this week happy   want this\n",
      "2. Similarity = 1.000\n",
      "  index=1743, hey thanks for being top new followers this week! much appreciated happy  want this ?\n",
      "  index=2622, hey thanks for being top new followers this week! much appreciated happy   want this ?\n",
      "3. Similarity = 1.000\n",
      "  index=1743, hey thanks for being top new followers this week! much appreciated happy  want this ?\n",
      "  index=2639, hey thanks for being top new followers this week! much appreciated happy   want this ?\n",
      "4. Similarity = 1.000\n",
      "  index=1751, thanks for connecting we are passionate about    hope you like our tweets happy\n",
      "  index=1967, thanks for connecting we are passionate about    hope you like our tweets happy\n",
      "5. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=1950, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
      "6. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=1976, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
      "7. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=2016, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
      "8. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=2032, thanks for the recent follow happy to connect happy  have a great thursday  want this\n",
      "9. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=2037, thanks for the recent follow happy to connect happy  have a great thursday   want this\n",
      "10. Similarity = 1.000\n",
      "  index=1780, thanks for the recent follow happy to connect happy  have a great thursday  (want this?\n",
      "  index=2068, thanks for the recent follow happy to connect happy  have a great thursday   want this ?\n",
      "\u001b[1maprroach: just tokenization without stopwords, mode:word counts\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=240, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "  index=328, thank you roshhh  i miss you and everyone too   love youuu more\n",
      "2. Similarity = 1.000\n",
      "  index=247, are dying of thirst  and it's all because of us unhappy  \n",
      "  index=379, are dying of thirst  and it's all because of us unhappy  \n",
      "3. Similarity = 1.000\n",
      "  index=1751, thanks for connecting we are passionate about    hope you like our tweets happy\n",
      "  index=1967, thanks for connecting we are passionate about    hope you like our tweets happy\n",
      "4. Similarity = 1.000\n",
      "  index=1838, share the love you're top engaged community members this week! much appreciated happy\n",
      "  index=2066, share the love you're top engaged community members this week! much appreciated happy\n",
      "5. Similarity = 1.000\n",
      "  index=1838, share the love you're top engaged community members this week! much appreciated happy\n",
      "  index=2235, share the love you're top engaged community members this week! much appreciated happy\n",
      "6. Similarity = 1.000\n",
      "  index=2066, share the love you're top engaged community members this week! much appreciated happy\n",
      "  index=2235, share the love you're top engaged community members this week! much appreciated happy\n",
      "7. Similarity = 1.000\n",
      "  index=2299, another great song  have a listen  its good! you may well like it happy\n",
      "  index=2303, another great song  have a listen  its good! you may well like it happy\n",
      "8. Similarity = 1.000\n",
      "  index=54, we don 27t talk anymore like we used to do unhappy\n",
      "  index=129, we don 27t talk anymore like we used to do unhappy\n",
      "9. Similarity = 1.000\n",
      "  index=141, her back unhappy \n",
      "  index=340, her back unhappy \n",
      "10. Similarity = 1.000\n",
      "  index=149, i miss you unhappy\n",
      "  index=756, miss you unhappy\n",
      "\u001b[1maprroach: just tokenization without stopwords, mode:TFIDF\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "2. Similarity = 1.000\n",
      "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "3. Similarity = 1.000\n",
      "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "4. Similarity = 1.000\n",
      "  index=1731, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "5. Similarity = 1.000\n",
      "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "6. Similarity = 1.000\n",
      "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "7. Similarity = 1.000\n",
      "  index=1747, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "8. Similarity = 1.000\n",
      "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "9. Similarity = 1.000\n",
      "  index=1900, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "10. Similarity = 1.000\n",
      "  index=2451, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n",
      "  index=2707, sixth spot not applicable team! higher pa!  )kisses thefashionicon\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + f'Top 10 similar pairs of tweets' + '\\033[0m')\n",
    "for approach in APPROACHES:\n",
    "    for mode in encoded_docs.columns.tolist():\n",
    "        top10 = get_top(encoded_docs.loc[approach, mode], df, 10)\n",
    "        print('\\033[1m' + f'aprroach: {approach}, mode:{mode}' + '\\033[0m')\n",
    "        for pair in enumerate(top10):\n",
    "            print(\"%.0f. Similarity = %.3f\" % (pair[0] + 1, pair[1][2]))\n",
    "            print(f'  index={pair[1][0]}, {df.loc[pair[1][0], \"Text\"]}')\n",
    "            print(f'  index={pair[1][1]}, {df.loc[pair[1][1], \"Text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba228b",
   "metadata": {
    "id": "d8ba228b"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4066458",
   "metadata": {
    "id": "e4066458"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(my_classes))\n",
    "    target_names = my_classes\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78837e71",
   "metadata": {
    "id": "78837e71"
   },
   "outputs": [],
   "source": [
    "def get_metrics(y_predicted, y_test):\n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                average='weighted')\n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                          average='weighted')\n",
    "\n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "\n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    accuracy, precision, recall, f1 = get_metrics(target, predictions)\n",
    "    print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" %\n",
    "          (accuracy, precision, recall, f1))\n",
    "    cm = confusion_matrix(target, predictions, labels=my_classes)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c7b02",
   "metadata": {
    "id": "468c7b02"
   },
   "source": [
    "#  Best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ed1c03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "14ed1c03",
    "outputId": "f90e9958-c9e3-4feb-e00a-7b415e468ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.932, precision = 0.935, recall = 0.932, f1 = 0.932\n",
      "confusion matrix\n",
      " [[154   6   7]\n",
      " [  0 174   3]\n",
      " [  2  19 177]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "approach = 'just tokenization without stopwords'\n",
    "mode = '0 or 1, if the word exists'\n",
    "evaluate_prediction(y_predicted.loc[approach, mode], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5eecd44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "f5eecd44",
    "outputId": "0df51aea-466b-479c-e82a-53f54aac1e58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e33da56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9e33da56",
    "outputId": "b3261587-f02e-4ba3-e39d-4a000b2fc849"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'how unhappy some dogs like it though talking to my over driver about where goinghe said he love to go to new york too but since trump it probably not does anybody know if the rand likely to fall against the dollar got some money need to change into but it keeps getting stronger unhappy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed[approach][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85a0666b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "85a0666b",
    "outputId": "cbc49b50-e375-4fcf-c6d9-9199f7358132"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'unhappi dog like though talk driver goingh said love go new york sinc trump probabl anybodi know rand like fall dollar got money need chang keep get stronger unhappi'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed['stemming'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a388ec9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "a388ec9b",
    "outputId": "626029ca-0446-4751-b9da-e8010e4ca7e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed['just tokenization'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c43e4c",
   "metadata": {
    "id": "51c43e4c"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e48e30",
   "metadata": {
    "id": "c8e48e30"
   },
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c9631c",
   "metadata": {
    "id": "90c9631c"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd16388b",
   "metadata": {
    "id": "fd16388b"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "cores = 1 if cores == 1 else cores - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144445c",
   "metadata": {
    "id": "1144445c"
   },
   "source": [
    "The parameters:\n",
    "* min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "* window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "* vector_size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6fe91",
   "metadata": {
    "id": "50f6fe91"
   },
   "source": [
    "## Building the Vocabulary Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7812d",
   "metadata": {
    "id": "71e7812d"
   },
   "source": [
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d074b01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "6d074b01",
    "outputId": "f53fdf21-5efb-4cb3-ae6e-5a7a6688e5cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach = 'just tokenization'\n",
    "text_processed[approach][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92d981e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d981e4",
    "outputId": "31d6f450-5d89-46de-9cc2-a29e700169e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 09:42:28,572 : INFO : collecting all words and their counts\n",
      "2021-09-28 09:42:28,574 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-09-28 09:42:28,585 : INFO : collected 6093 word types from a corpus of 20150 raw words and 2708 sentences\n",
      "2021-09-28 09:42:28,586 : INFO : Creating a fresh vocabulary\n",
      "2021-09-28 09:42:28,616 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6093 unique words (100.0%% of original 6093, drops 0)', 'datetime': '2021-09-28T09:42:28.616309', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-09-28 09:42:28,619 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20150 word corpus (100.0%% of original 20150, drops 0)', 'datetime': '2021-09-28T09:42:28.619060', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-09-28 09:42:28,664 : INFO : deleting the raw counts dictionary of 6093 items\n",
      "2021-09-28 09:42:28,666 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2021-09-28 09:42:28,669 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 18002.20255853385 word corpus (89.3%% of prior 20150)', 'datetime': '2021-09-28T09:42:28.669223', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-09-28 09:42:28,746 : INFO : estimated required memory for 6093 words and 2000 dimensions: 100534500 bytes\n",
      "2021-09-28 09:42:28,748 : INFO : resetting layer weights\n",
      "2021-09-28 09:42:28,841 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-09-28T09:42:28.841006', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2021-09-28 09:42:28,843 : INFO : Word2Vec lifecycle event {'msg': 'training model with 0 workers on 6093 vocabulary and 2000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-09-28T09:42:28.843820', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2021-09-28 09:42:28,853 : INFO : EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-09-28 09:42:28,854 : WARNING : EPOCH - 1 : supplied example count (0) did not equal expected count (2708)\n",
      "2021-09-28 09:42:28,858 : WARNING : EPOCH - 1 : supplied raw word count (0) did not equal expected count (20150)\n",
      "2021-09-28 09:42:28,866 : INFO : EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-09-28 09:42:28,867 : WARNING : EPOCH - 2 : supplied example count (0) did not equal expected count (2708)\n",
      "2021-09-28 09:42:28,869 : WARNING : EPOCH - 2 : supplied raw word count (0) did not equal expected count (20150)\n",
      "2021-09-28 09:42:28,882 : INFO : EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-09-28 09:42:28,888 : WARNING : EPOCH - 3 : supplied example count (0) did not equal expected count (2708)\n",
      "2021-09-28 09:42:28,895 : WARNING : EPOCH - 3 : supplied raw word count (0) did not equal expected count (20150)\n",
      "2021-09-28 09:42:28,911 : INFO : EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-09-28 09:42:28,913 : WARNING : EPOCH - 4 : supplied example count (0) did not equal expected count (2708)\n",
      "2021-09-28 09:42:28,914 : WARNING : EPOCH - 4 : supplied raw word count (0) did not equal expected count (20150)\n",
      "2021-09-28 09:42:28,923 : INFO : EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-09-28 09:42:28,924 : WARNING : EPOCH - 5 : supplied example count (0) did not equal expected count (2708)\n",
      "2021-09-28 09:42:28,927 : WARNING : EPOCH - 5 : supplied raw word count (0) did not equal expected count (20150)\n",
      "2021-09-28 09:42:28,930 : INFO : Word2Vec lifecycle event {'msg': 'training on 0 raw words (0 effective words) took 0.1s, 0 effective words/s', 'datetime': '2021-09-28T09:42:28.930558', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2021-09-28 09:42:28,932 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=6093, vector_size=2000, alpha=0.025)', 'datetime': '2021-09-28T09:42:28.932541', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "sentences = [sentence.split() for sentence in text_processed[approach]]\n",
    "w2v_model = Word2Vec(sentences, vector_size=2000, min_count=1, workers=cores-1)\n",
    "t = round((time() - t) / 60, 2)\n",
    "print(f'Time to build vocab: {t} mins')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2195d817",
   "metadata": {
    "id": "2195d817"
   },
   "source": [
    "Example vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "486432b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "486432b3",
    "outputId": "2ba1b380-8c11-401a-c11f-f8cd81654624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'unhappy', 'thanks', 'want', \"n't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index_to_key[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b932e4",
   "metadata": {
    "id": "28b932e4"
   },
   "source": [
    "Example vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a4f190d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a4f190d",
    "outputId": "863232f1-c7e3-4457-abfc-913816a263b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.6811362e-05,  1.1821508e-05,  2.5516748e-04, ...,\n",
       "        3.6566341e-04,  2.7327836e-04,  4.6247302e-04], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = w2v_model.wv.index_to_key[0:10][0]\n",
    "print(word)\n",
    "w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2ed07",
   "metadata": {
    "id": "b8c2ed07"
   },
   "source": [
    "Now we have a vector for each word. How do we get a vector for a sequence of words (aka a document)?\n",
    "The most naive way is just to take an average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f0431c8",
   "metadata": {
    "id": "7f0431c8"
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    for word in words:\n",
    "        if word in wv.index_to_key:\n",
    "            if isinstance(wv[word], np.ndarray):\n",
    "                mean.append(wv[word])\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(\n",
    "        np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c53d6408",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c53d6408",
    "outputId": "7170feda-bedf-46ca-a7ea-bea0f69a19a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 721 ms, sys: 8.84 ms, total: 730 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "X_word_average = word_averaging_list(w2v_model.wv, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea64d972",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea64d972",
    "outputId": "8fbb89a5-d06d-4b06-fbb2-40799a6faa3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 10.0, 'penalty': 'l2'}\n",
      "accuracy:  0.905357541958898\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_word_average, list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
    "# l1 lasso l2 ridge\n",
    "params = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "lr_gs = GridSearchCV(LogisticRegression(), params, cv=5,\n",
    "                     n_jobs=-1, scoring='accuracy')\n",
    "lr_gs.fit(X_train, y_train)\n",
    "print('best parameters: ', lr_gs.best_params_)\n",
    "print('accuracy: ', lr_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2tA9K5tGU1v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "c2tA9K5tGU1v",
    "outputId": "72e01013-9083-4c28-fa85-63a12d1ca28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.902, precision = 0.908, recall = 0.902, f1 = 0.901\n",
      "confusion matrix\n",
      " [[151  10   6]\n",
      " [  0 171   6]\n",
      " [  2  29 167]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=lr_gs.best_params_[\n",
    "                        'C'], penalty=lr_gs.best_params_['penalty'])\n",
    "lr.fit(X_train, y_train)\n",
    "y_predicted_w2v = lr.predict(X_test)\n",
    "evaluate_prediction(y_predicted_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c548b79a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c548b79a",
    "outputId": "9cef766a-6cd0-4ee0-ce73-15c6edfb1fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTop 10 similar pairs of tweets\u001b[0m\n",
      "\u001b[1maprroach: Word2Vec\u001b[0m\n",
      "1. Similarity = 1.000\n",
      "  index=1573, aiadmk expels after supreme court ruling knocks out of cm race \n",
      "  index=1574, aiadmk expels after supreme court ruling knocks out of cm race \n",
      "2. Similarity = 1.000\n",
      "  index=110, poor baby unhappy  i hate people\n",
      "  index=417, poor baby unhappy  i hate people\n",
      "3. Similarity = 1.000\n",
      "  index=549, roc boyz sad \n",
      "  index=630, roc boyz sad \n",
      "4. Similarity = 1.000\n",
      "  index=549, roc boyz sad \n",
      "  index=690, roc boyz sad \n",
      "5. Similarity = 1.000\n",
      "  index=549, roc boyz sad \n",
      "  index=691, roc boyz sad \n",
      "6. Similarity = 1.000\n",
      "  index=630, roc boyz sad \n",
      "  index=690, roc boyz sad \n",
      "7. Similarity = 1.000\n",
      "  index=630, roc boyz sad \n",
      "  index=691, roc boyz sad \n",
      "8. Similarity = 1.000\n",
      "  index=690, roc boyz sad \n",
      "  index=691, roc boyz sad \n",
      "9. Similarity = 1.000\n",
      "  index=1802, mutual  let's talk more!me  for sure!mutual   happy  me \n",
      "  index=1896, mutual  let's talk more!me  for sure!mutual   happy  me \n",
      "10. Similarity = 1.000\n",
      "  index=1802, mutual  let's talk more!me  for sure!mutual   happy  me \n",
      "  index=1968, mutual  let's talk more!me  for sure!mutual   happy  me \n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + f'Top 10 similar pairs of tweets' + '\\033[0m')\n",
    "top10 = get_top(X_word_average, df, 10)\n",
    "print('\\033[1m' + f'aprroach: Word2Vec' + '\\033[0m')\n",
    "for pair in enumerate(top10):\n",
    "    print(\"%.0f. Similarity = %.3f\" % (pair[0] + 1, pair[1][2]))\n",
    "    print(f'  index={pair[1][0]}, {df.loc[pair[1][0], \"Text\"]}')\n",
    "    print(f'  index={pair[1][1]}, {df.loc[pair[1][1], \"Text\"]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
