{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6b483e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from time import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from nltk.metrics.distance  import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf0e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/'\n",
    "NLTK_PATH = './nltk_data/'\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e29616",
   "metadata": {},
   "source": [
    "# Replace unencodable character by questionmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd93805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_characters(input_file, output_file):    \n",
    "    with open(file=PATH+input_file, mode=\"r\", encoding='utf-8', errors='replace') as f_in:\n",
    "        data = f_in.read()\n",
    "        with open(file=PATH+output_file, mode=\"w\", encoding='utf-8') as f_out:\n",
    "            f_out.write(data)\n",
    "\n",
    "sanitize_characters(\"processedNegative.csv\", \"negative_clean\")\n",
    "sanitize_characters(\"processedNeutral.csv\", \"neutral_clean\")\n",
    "sanitize_characters(\"processedPositive.csv\", \"positive_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed46715",
   "metadata": {},
   "source": [
    "# Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a14393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_file, sep, class_label):\n",
    "    with open(PATH+input_file, 'r') as f_in:\n",
    "        text = f_in.read()\n",
    "    df = pd.DataFrame(text.split(sep), columns=['Text'])\n",
    "    df['Class'] = class_label\n",
    "    return df\n",
    "\n",
    "negative = get_data(\"processedNegative.csv\", ' ,', -1)\n",
    "neutral = get_data(\"processedNeutral.csv\", ' ,', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9928e7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  \\\n",
       "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy   \n",
       "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy   \n",
       "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy   \n",
       "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy   \n",
       "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad   \n",
       "\n",
       "   Class  \n",
       "0     -1  \n",
       "1     -1  \n",
       "2     -1  \n",
       "3     -1  \n",
       "4     -1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d37c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pak PM survives removal scare, but court orders further probe into corruption charge.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why doctors, pharma companies are opposing names on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Text  \\\n",
       "0                                                                                               Pak PM survives removal scare, but court orders further probe into corruption charge.   \n",
       "1  Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.,Art of Living's fights back over Yamuna floodplain damage, livid.   \n",
       "2                                                                                                       FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign?   \n",
       "3                                                                                                                                 Why doctors, pharma companies are opposing names on   \n",
       "4                                                                                       Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river.   \n",
       "\n",
       "   Class  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8edc3",
   "metadata": {},
   "source": [
    "## Save positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf69643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_beginspace(lst):\n",
    "    new_lst = []\n",
    "    i = 0\n",
    "    change = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == '':\n",
    "            i += 1\n",
    "            continue\n",
    "        if i < len(lst)-1:\n",
    "            if lst[i+1] == '':\n",
    "                i += 1\n",
    "                continue\n",
    "            if lst[i+1][0] == ' ':\n",
    "                new_lst.append(lst[i]+lst[i+1])\n",
    "                i += 2\n",
    "                change = 1\n",
    "                continue\n",
    "        new_lst.append(lst[i])\n",
    "        i += 1\n",
    "    return(new_lst, change)\n",
    "\n",
    "\n",
    "def join_endspace(lst):\n",
    "    new_lst = []\n",
    "    i = 0\n",
    "    change = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == '':\n",
    "            i += 1\n",
    "            continue\n",
    "        if i+1 < len(lst):\n",
    "            if lst[i+1] == '':\n",
    "                i += 1\n",
    "                continue\n",
    "            if (lst[i][-1] == ' '):\n",
    "                new_lst.append(lst[i]+lst[i+1])\n",
    "                i += 2\n",
    "                change = 1\n",
    "                continue;\n",
    "        new_lst.append(lst[i])\n",
    "        i += 1\n",
    "    return(new_lst, change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d407c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(input_file, sep):\n",
    "    with open(PATH+input_file, 'r') as f_in:\n",
    "        text = f_in.read()\n",
    "    return text.split(sep)\n",
    "\n",
    "lst = get_list(\"processedPositive.csv\", ',')\n",
    "change = 1\n",
    "while(change == 1):\n",
    "    lst, change = join_beginspace(lst)\n",
    "change = 1\n",
    "while(change == 1):\n",
    "    lst, change = join_endspace(lst)\n",
    "positive = pd.DataFrame(lst, columns=['Text'])\n",
    "positive['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82c188d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good luck to Rich riding for great project in this Sunday. Can you donate?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Omg he... kissed... him crying with joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     Text  \\\n",
       "0          An inspiration in all aspects: Fashion fitness beauty and personality. :)KISSES TheFashionIcon   \n",
       "1  Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...   \n",
       "2  Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage   \n",
       "3                              Good luck to Rich riding for great project in this Sunday. Can you donate?   \n",
       "4                                                                 Omg he... kissed... him crying with joy   \n",
       "\n",
       "   Class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31b7d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  \\\n",
       "0  How unhappy  some dogs like it though,talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not,Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy   \n",
       "1                                                                                                                                                                                                                                                                          I miss going to gigs in Liverpool unhappy   \n",
       "2                                                                                                                                                                                                                                                                       There isnt a new Riverdale tonight ? unhappy   \n",
       "3                                                                                                                                                                                                          it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy   \n",
       "4                                                                                                                                                                Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu,don't like how jittery caffeine makes me sad   \n",
       "\n",
       "   Class  \n",
       "0     -1  \n",
       "1     -1  \n",
       "2     -1  \n",
       "3     -1  \n",
       "4     -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([negative, neutral, positive], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f235e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>- top engaged members this week happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam crying with joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>Great! You're welcome Josh happy  ^Adam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      Text  \\\n",
       "2703  Thanks for the recent follow Happy to connect happy  have a great Thursday. Get this   \n",
       "2704                                                 - top engaged members this week happy   \n",
       "2705                              ngam to  weeks left for cadet pilot exam crying with joy   \n",
       "2706                                               Great! You're welcome Josh happy  ^Adam   \n",
       "2707                    Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon   \n",
       "\n",
       "      Class  \n",
       "2703      1  \n",
       "2704      1  \n",
       "2705      1  \n",
       "2706      1  \n",
       "2707      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f237278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.057976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.819033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Class\n",
       "count  2708.000000\n",
       "mean      0.057976\n",
       "std       0.819033\n",
       "min      -1.000000\n",
       "25%      -1.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034b126",
   "metadata": {},
   "source": [
    "# cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42841448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there isnt a new riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                Text  \\\n",
       "0  how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy   \n",
       "1                                                                                                                                                                                                                                                                          i miss going to gigs in liverpool unhappy   \n",
       "2                                                                                                                                                                                                                                                                       there isnt a new riverdale tonight ? unhappy   \n",
       "3                                                                                                                                                                                                          it's that a dy guy from pop asia and then the translator so they'll probs go with them around aus unhappy   \n",
       "4                                                                                                                                                                who's that chair you're sitting in? is this how i find out  everyone knows now  you've shamed me in pu,don't like how jittery caffeine makes me sad   \n",
       "\n",
       "   Class  \n",
       "0     -1  \n",
       "1     -1  \n",
       "2     -1  \n",
       "3     -1  \n",
       "4     -1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\",,\", \",\", regex=True)\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, 'Text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f02ae",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61175e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text\n",
       "Class      \n",
       "-1      834\n",
       " 0      883\n",
       " 1      991"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classes = df.Class.unique()\n",
    "df.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f730a9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d727b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, approach):\n",
    "    if approach=='stemming':\n",
    "        ps =PorterStemmer()\n",
    "    elif approach=='lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    elif approach=='stemming+misspelling':\n",
    "        ps = PorterStemmer()\n",
    "        correct_spellings = words.words()\n",
    "    elif approach=='lemmatization+misspelling':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        correct_spellings = words.words()\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in stopwords.words('english'):\n",
    "                continue\n",
    "            if word[0] == \"'\" or word[0] in ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'):\n",
    "                continue\n",
    "            \n",
    "            if approach=='stemming':\n",
    "                tokens.append(ps.stem(word))\n",
    "            elif approach=='lemmatization':\n",
    "                tokens.append(lemmatizer.lemmatize(word))\n",
    "            elif approach=='stemming+misspelling':\n",
    "                after_stem = ps.stem(word)\n",
    "                if after_stem in correct_spellings:\n",
    "                    tokens.append(after_stem)\n",
    "                else:\n",
    "                    after_correct = [\n",
    "                        (jaccard_distance(set(ngrams(after_stem, 2)), \n",
    "                                          set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==after_stem[0]\n",
    "                        ]\n",
    "                    if after_correct:\n",
    "                        tokens.append(sorted(after_correct, key = lambda val:val[0])[0][1])\n",
    "                    else:\n",
    "                        print(word, after_stem)\n",
    "                        tokens.append(after_stem)\n",
    "            elif approach=='lemmatization+misspelling':\n",
    "                after_lemm = lemmatizer.lemmatize(word)\n",
    "                if after_lemm in correct_spellings:\n",
    "                    tokens.append(after_lemm)\n",
    "                else:\n",
    "                    after_correct = [\n",
    "                        (jaccard_distance(set(ngrams(after_lemm, 2)), \n",
    "                                          set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==after_lemm[0]\n",
    "                        ]\n",
    "                    if after_correct:\n",
    "                        tokens.append(sorted(after_correct, key = lambda val:val[0])[0][1])\n",
    "                    else:\n",
    "                        print(word, after_lemm)\n",
    "                        tokens.append(after_lemm)\n",
    "\n",
    "            else:\n",
    "                tokens.append(word)\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fa0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(sentence, mode):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(sentence)\n",
    "    return t.texts_to_matrix(sentence, mode=mode)#'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5825acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPROACHES = ['just tokenization', 'stemming', 'lemmatization', \n",
    "            'stemming+misspelling', 'lemmatization+misspelling']\n",
    "encoded_docs = pd.DataFrame(data = None, \n",
    "                            index=APPROACHES, \n",
    "                            columns=['0 or 1, if the word exists', 'word counts', 'TFIDF'])\n",
    "accuracy_matrix = encoded_docs.copy()\n",
    "roc_auc_matrix = encoded_docs.copy()\n",
    "y_predicted = encoded_docs.copy()\n",
    "mapping_columns = {'0 or 1, if the word exists':'binary', 'word counts':'count', 'TFIDF':'tfidf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e7216e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./nltk_data/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to ./nltk_data/...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append(NLTK_PATH)\n",
    "nltk.download('punkt', download_dir=NLTK_PATH)\n",
    "nltk.download('stopwords', download_dir=NLTK_PATH)\n",
    "nltk.download('wordnet', download_dir=NLTK_PATH)\n",
    "nltk.download('words', download_dir=NLTK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a35b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time get word matrix just tokenization: 0.05 mins\n",
      "Time get word matrix stemming: 0.06 mins\n",
      "Time get word matrix lemmatization: 0.08 mins\n",
      ",77 ,77\n",
      ",22 ,22\n",
      ",7 ,7\n",
      ",63 ,63\n",
      "Time get word matrix stemming+misspelling: 10.37 mins\n",
      ",77 ,77\n",
      ",22 ,22\n",
      ",7 ,7\n",
      ",63 ,63\n",
      "Time get word matrix lemmatization+misspelling: 6.36 mins\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "list_labels = df['Class'].tolist()\n",
    "text_processed = dict()\n",
    "for approach in APPROACHES:\n",
    "    t = time()\n",
    "    text_processed[approach] = df.apply(lambda r: ' '.join(tokenize_text(r['Text'], approach)), axis=1).values\n",
    "    t = round((time() - t) / 60, 2)\n",
    "    print(f'Time get word matrix {approach}: {t} mins')\n",
    "    for mode in encoded_docs.columns.tolist():\n",
    "        encoded_docs.loc[approach,mode]=get_matrix(text_processed[approach], mapping_columns[mode])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(encoded_docs.loc[approach,mode], list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_predicted_tmp = lr.predict(X_test)\n",
    "        y_predicted.loc[approach,mode] = y_predicted_tmp\n",
    "        accuracy_matrix.loc[approach,mode] = round(accuracy_score(y_test, y_predicted_tmp), 3)\n",
    "        roc_auc_matrix.loc[approach,mode] = round(roc_auc_score(label_binarize(y_test, classes=my_classes),\n",
    "                                                                label_binarize(y_predicted_tmp, classes=my_classes),\n",
    "                                                                average='macro',\n",
    "                                                                multi_class='ovo'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "036aaf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.915</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.917</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.915</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming+misspelling</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization+misspelling</th>\n",
       "      <td>0.921</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0 or 1, if the word exists word counts  TFIDF\n",
       "just tokenization                              0.915       0.913  0.902\n",
       "stemming                                       0.917       0.915  0.902\n",
       "lemmatization                                  0.915       0.911    0.9\n",
       "stemming+misspelling                           0.924       0.919  0.899\n",
       "lemmatization+misspelling                      0.921       0.915  0.906"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65c06770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming+misspelling</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization+misspelling</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0 or 1, if the word exists word counts  TFIDF\n",
       "just tokenization                              0.938       0.936  0.927\n",
       "stemming                                       0.939       0.937  0.927\n",
       "lemmatization                                  0.938       0.935  0.925\n",
       "stemming+misspelling                           0.944        0.94  0.924\n",
       "lemmatization+misspelling                      0.941       0.937  0.929"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba228b",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4066458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(my_classes))\n",
    "    target_names = my_classes\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78837e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_predicted, y_test):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    accuracy, precision, recall, f1 = get_metrics(target, predictions)\n",
    "    print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "#     print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions, labels=my_classes)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c7b02",
   "metadata": {},
   "source": [
    "#  Best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ea566a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach ='stemming+misspelling'\n",
    "mode = '0 or 1, if the word exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14ed1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.924, precision = 0.930, recall = 0.924, f1 = 0.924\n",
      "confusion matrix\n",
      " [[152   9   6]\n",
      " [  1 175   1]\n",
      " [  1  23 174]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "approach ='stemming+misspelling'\n",
    "mode = '0 or 1, if the word exists'\n",
    "evaluate_prediction(y_predicted.loc[approach, mode], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5eecd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how unhappy  some dogs like it though,talking to my over driver about where i'm goinghe said he'd love to go to new york too but since trump it's probably not,does anybody know if the rand's likely to fall against the dollar? i got some money  i need to change into r but it keeps getting stronger unhappy\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e33da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unhappily dog like though talk driver going said love go new york since trump probabl anybody know rand like fall dollar got money need chang keep get strong unhappily'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed[approach][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85a0666b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unhappi dog like though talk driver goingh said love go new york sinc trump probabl anybodi know rand like fall dollar got money need chang keep get stronger unhappi'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed['stemming'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a388ec9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed['just tokenization'][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c43e4c",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e48e30",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90c9631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd16388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "cores = 1 if cores == 1 else cores - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144445c",
   "metadata": {},
   "source": [
    "The parameters:\n",
    "* min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "* window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "* vector_size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6fe91",
   "metadata": {},
   "source": [
    "## Building the Vocabulary Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7812d",
   "metadata": {},
   "source": [
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6d074b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unhappy dogs like though talking driver goinghe said love go new york since trump probably anybody know rand likely fall dollar got money need change keeps getting stronger unhappy'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach = 'just tokenization'\n",
    "text_processed[approach][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92d981e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 11:11:11,086 : INFO : collecting all words and their counts\n",
      "2021-09-26 11:11:11,087 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-09-26 11:11:11,092 : INFO : collected 6091 word types from a corpus of 20149 raw words and 2708 sentences\n",
      "2021-09-26 11:11:11,092 : INFO : Creating a fresh vocabulary\n",
      "2021-09-26 11:11:11,106 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6091 unique words (100.0%% of original 6091, drops 0)', 'datetime': '2021-09-26T11:11:11.106731', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2021-09-26 11:11:11,107 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20149 word corpus (100.0%% of original 20149, drops 0)', 'datetime': '2021-09-26T11:11:11.107593', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2021-09-26 11:11:11,136 : INFO : deleting the raw counts dictionary of 6091 items\n",
      "2021-09-26 11:11:11,137 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2021-09-26 11:11:11,138 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 18001.123755222172 word corpus (89.3%% of prior 20149)', 'datetime': '2021-09-26T11:11:11.138152', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "2021-09-26 11:11:11,194 : INFO : estimated required memory for 6091 words and 2000 dimensions: 100501500 bytes\n",
      "2021-09-26 11:11:11,195 : INFO : resetting layer weights\n",
      "2021-09-26 11:11:11,241 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-09-26T11:11:11.241123', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "2021-09-26 11:11:11,241 : INFO : Word2Vec lifecycle event {'msg': 'training model with 1 workers on 6091 vocabulary and 2000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-09-26T11:11:11.241925', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2021-09-26 11:11:11,425 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-09-26 11:11:11,426 : INFO : EPOCH - 1 : training on 20149 raw words (17996 effective words) took 0.2s, 99204 effective words/s\n",
      "2021-09-26 11:11:11,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-09-26 11:11:11,574 : INFO : EPOCH - 2 : training on 20149 raw words (17974 effective words) took 0.1s, 124187 effective words/s\n",
      "2021-09-26 11:11:11,720 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-09-26 11:11:11,721 : INFO : EPOCH - 3 : training on 20149 raw words (17996 effective words) took 0.1s, 125928 effective words/s\n",
      "2021-09-26 11:11:11,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-09-26 11:11:11,867 : INFO : EPOCH - 4 : training on 20149 raw words (17972 effective words) took 0.1s, 124774 effective words/s\n",
      "2021-09-26 11:11:12,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-09-26 11:11:12,016 : INFO : EPOCH - 5 : training on 20149 raw words (18052 effective words) took 0.1s, 124399 effective words/s\n",
      "2021-09-26 11:11:12,016 : INFO : Word2Vec lifecycle event {'msg': 'training on 100745 raw words (89990 effective words) took 0.8s, 116244 effective words/s', 'datetime': '2021-09-26T11:11:12.016632', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "2021-09-26 11:11:12,017 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=6091, vector_size=2000, alpha=0.025)', 'datetime': '2021-09-26T11:11:12.017163', 'gensim': '4.1.1', 'python': '3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:46) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.25-linuxkit-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "sentences = [sentence.split() for sentence in text_processed[approach]]\n",
    "w2v_model = Word2Vec(sentences, vector_size=2000, min_count = 1, workers=cores-1)\n",
    "t = round((time() - t) / 60, 2)\n",
    "print(f'Time to build vocab: {t} mins')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2195d817",
   "metadata": {},
   "source": [
    "Example vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "486432b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'unhappy', 'thanks', 'want', \"n't\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index_to_key[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b932e4",
   "metadata": {},
   "source": [
    "Example vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a4f190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.1831524e-03, -2.6535292e-04,  1.9717871e-03, ...,\n",
       "       -1.1191389e-03,  5.7883642e-04, -2.2107408e-05], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = w2v_model.wv.index_to_key[0:10][0]\n",
    "print(word)\n",
    "w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2ed07",
   "metadata": {},
   "source": [
    "Now we have a vector for each word. How do we get a vector for a sequence of words (aka a document)?\n",
    "The most naive way is just to take an average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f0431c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wv.index_to_key:\n",
    "            if isinstance(wv[word], np.ndarray):\n",
    "                mean.append(wv[word])\n",
    "#         elif word in wv.vocab:\n",
    "#             mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "#             all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c53d6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 11:11:28,171 : WARNING : cannot compute similarity with no input []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 444 ms, sys: 4.88 ms, total: 449 ms\n",
      "Wall time: 450 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_word_average = word_averaging_list(w2v_model.wv, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea64d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.887, precision = 0.897, recall = 0.887, f1 = 0.887\n",
      "confusion matrix\n",
      " [[149  10   8]\n",
      " [  3 172   2]\n",
      " [  6  32 160]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_word_average, list_labels, stratify=list_labels, test_size=0.2, random_state=42)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_predicted_w2v = lr.predict(X_test)\n",
    "evaluate_prediction(y_predicted_w2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548b79a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
